# KeyError 'roc_auc' ë²„ê·¸ ìˆ˜ì • ì™„ë£Œ

## ë°œê²¬ëœ ì—ëŸ¬

```python
KeyError: 'roc_auc'
File "/home/ykio/notebooks/claude/multivariate_mae_experiments.py", line 1072, in run_single_experiment
    print(f"\nResults: ROC-AUC={metrics['roc_auc']:.4f}, F1={metrics['f1_score']:.4f}")
```

## ì›ì¸ ë¶„ì„

### ê·¼ë³¸ ì›ì¸
`Evaluator.evaluate()` ë©”ì„œë“œê°€ ë°˜í™˜í•˜ëŠ” `metrics` ë”•ì…”ë„ˆë¦¬ëŠ” 3-way evaluation êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:

```python
{
    'sequence': {'roc_auc': ..., 'f1_score': ..., 'precision': ..., 'recall': ..., 'optimal_threshold': ...},
    'point': {'roc_auc': ..., 'f1_score': ..., 'precision': ..., 'recall': ..., 'optimal_threshold': ...},
    'combined': {'roc_auc': ..., 'f1_score': ..., 'seq_weight': ..., 'point_weight': ...}
}
```

### ì™œ ë°œìƒí–ˆë‚˜?
1. Point-level anomaly detection ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ë©´ì„œ `Evaluator.evaluate()`ê°€ 3-way evaluationì„ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •ë¨
2. `ExperimentRunner.run_single_experiment()` ë©”ì„œë“œëŠ” ì—¬ì „íˆ flat dictionaryë¥¼ ê°€ì •í•˜ê³  `metrics['roc_auc']`ë¡œ ì ‘ê·¼
3. ì‹¤ì œë¡œëŠ” `metrics['combined']['roc_auc']`ë¡œ ì ‘ê·¼í•´ì•¼ í•¨

### ê¸°ì¡´ ì½”ë“œì˜ ë¬¸ì œ
```python
# Line 1072
print(f"\nResults: ROC-AUC={metrics['roc_auc']:.4f}, F1={metrics['f1_score']:.4f}")
# âŒ KeyError: 'roc_auc' - metricsëŠ” nested dictionary
```

---

## ìˆ˜ì • ë‚´ìš©

### ìˆ˜ì •ëœ íŒŒì¼
**multivariate_mae_experiments.py**
- Lines 1071-1075: `run_single_experiment()` ë©”ì„œë“œ
- Lines 1211-1234: `_plot_hyperparameter_comparison()` ë©”ì„œë“œ
- Lines 1236-1260: `_plot_ablation_comparison()` ë©”ì„œë“œ
- Lines 1300-1323: `_plot_performance_heatmap()` ë©”ì„œë“œ

### ìˆ˜ì • ì „ (run_single_experiment)
```python
self.results.append(result)
print(f"\nResults: ROC-AUC={metrics['roc_auc']:.4f}, F1={metrics['f1_score']:.4f}")

return result
```

### ìˆ˜ì • í›„ (run_single_experiment)
```python
self.results.append(result)
print(f"\nResults: ROC-AUC={metrics['combined']['roc_auc']:.4f}, F1={metrics['combined']['f1_score']:.4f}")
print(f"  Sequence-Level: ROC-AUC={metrics['sequence']['roc_auc']:.4f}, F1={metrics['sequence']['f1_score']:.4f}")
print(f"  Point-Level: ROC-AUC={metrics['point']['roc_auc']:.4f}, F1={metrics['point']['f1_score']:.4f}")

return result
```

### ìˆ˜ì • ì „ (Visualization methods)
```python
# In _plot_hyperparameter_comparison, _plot_ablation_comparison, _plot_performance_heatmap
values = [r['metrics'][metric] for r in hyperparameter_results]
```

### ìˆ˜ì • í›„ (Visualization methods)
```python
# In _plot_hyperparameter_comparison, _plot_ablation_comparison, _plot_performance_heatmap
values = [r['metrics']['combined'][metric] for r in hyperparameter_results]
```

### ë³€ê²½ ì‚¬í•­
1. **run_single_experiment()**: `metrics['roc_auc']` â†’ `metrics['combined']['roc_auc']`
2. **run_single_experiment()**: Sequence-level ë° Point-level ê²°ê³¼ë„ í•¨ê»˜ ì¶œë ¥
3. **_plot_hyperparameter_comparison()**: Combined metrics ì‚¬ìš©
4. **_plot_ablation_comparison()**: Combined metrics ì‚¬ìš©
5. **_plot_performance_heatmap()**: Combined metrics ì‚¬ìš©

---

## ê²€ì¦

### í…ŒìŠ¤íŠ¸ 1: Experiment Runner ë‹¨ë… í…ŒìŠ¤íŠ¸

**íŒŒì¼**: `test_experiment_fix.py`

```bash
python test_experiment_fix.py
```

**ê²°ê³¼**:
```
================================================================================
EVALUATION RESULTS
================================================================================

Metric               Sequence-Level       Point-Level          Combined
--------------------------------------------------------------------------------
ROC-AUC              0.5867               0.4865               0.5592
Precision            0.2727               0.0818               0.2204
Recall               0.6000               0.5256               0.5796
F1-Score             0.3750               0.1415               0.3110
================================================================================
Combined weights: Sequence=0.73, Point=0.27
================================================================================

Results: ROC-AUC=0.5592, F1=0.3110
  Sequence-Level: ROC-AUC=0.5867, F1=0.3750
  Point-Level: ROC-AUC=0.4865, F1=0.1415

âœ… All checks passed!
TEST PASSED - No KeyError!
```

### í…ŒìŠ¤íŠ¸ 2: Visualization í…ŒìŠ¤íŠ¸

**íŒŒì¼**: `test_visualization_fix.py`

```bash
python test_visualization_fix.py
```

**ê²°ê³¼**:
```
================================================================================
GENERATING VISUALIZATIONS
================================================================================
âœ“ Saved hyperparameter_comparison.png
âœ“ Saved ablation_comparison.png
âœ“ Saved training_curves.png
âœ“ ROC comparison (requires FPR/TPR data - skipped for now)
âœ“ Saved performance_heatmap.png

âœ… All visualizations generated successfully!
âœ… No KeyError!
```

### í…ŒìŠ¤íŠ¸ 3: ì „ì²´ ì‹¤í—˜ ì‹¤í–‰

```bash
python multivariate_mae_experiments.py
```

**ê²°ê³¼**: âœ… ì—ëŸ¬ ì—†ì´ ì •ìƒ ì‹¤í–‰ë¨ (ê²°ê³¼ ì¶œë ¥ ë° ì‹œê°í™” ëª¨ë‘ ì„±ê³µ)

---

## ì˜í–¥ ë²”ìœ„

### ìˆ˜ì •ëœ ê¸°ëŠ¥
1. **ExperimentRunner.run_single_experiment()** - ì‹¤í—˜ ê²°ê³¼ ì¶œë ¥ ë°©ì‹ ê°œì„ 
   - Combined metricsë¥¼ ë©”ì¸ìœ¼ë¡œ ì¶œë ¥
   - Sequence-levelê³¼ Point-level ê²°ê³¼ë„ í•¨ê»˜ í‘œì‹œ

2. **ExperimentRunner._plot_hyperparameter_comparison()** - ì‹œê°í™” ìˆ˜ì •
   - Combined metrics ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½

3. **ExperimentRunner._plot_ablation_comparison()** - ì‹œê°í™” ìˆ˜ì •
   - Combined metrics ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½

4. **ExperimentRunner._plot_performance_heatmap()** - ì‹œê°í™” ìˆ˜ì •
   - Combined metrics ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½

### ì˜í–¥ë°›ì§€ ì•ŠëŠ” ê¸°ëŠ¥
- Dataset ìƒì„±
- Model í•™ìŠµ
- Evaluation ê³„ì‚°
- JSON ì €ì¥ (ì´ë¯¸ ìˆ˜ì •ë¨)
- Training curves ì‹œê°í™” (history ë°ì´í„° ì‚¬ìš©, metrics ë¬´ê´€)

---

## ê²°ê³¼

### âœ… ì™„ì „íˆ í•´ê²°ë¨

1. **KeyError ìˆ˜ì •**: Nested metrics êµ¬ì¡°ì— ë§ê²Œ ì ‘ê·¼ ë°©ì‹ ìˆ˜ì •
2. **ì¶œë ¥ ê°œì„ **: 3ê°€ì§€ ë ˆë²¨(Sequence/Point/Combined)ì˜ ê²°ê³¼ë¥¼ ëª¨ë‘ í‘œì‹œ
3. **ì‹œê°í™” ìˆ˜ì •**: ëª¨ë“  visualization ë©”ì„œë“œê°€ Combined metrics ì‚¬ìš©
4. **Backward compatible**: ê¸°ì¡´ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ êµ¬ì¡°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
5. **í…ŒìŠ¤íŠ¸ ì™„ë£Œ**: ë‹¨ë… í…ŒìŠ¤íŠ¸, ì‹œê°í™” í…ŒìŠ¤íŠ¸, ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ ëª¨ë‘ ì„±ê³µ

### ì¶œë ¥ ì˜ˆì‹œ
```
Results: ROC-AUC=0.5592, F1=0.3110
  Sequence-Level: ROC-AUC=0.5867, F1=0.3750
  Point-Level: ROC-AUC=0.4865, F1=0.1415
```

---

## ê´€ë ¨ ë²„ê·¸ ìˆ˜ì •

### ì´ì „ ìˆ˜ì • ì‚¬í•­
1. **JSON Serialization ë²„ê·¸** (2024-12-30)
   - íŒŒì¼: [BUGFIX_SUMMARY.md](BUGFIX_SUMMARY.md)
   - ë¬¸ì œ: NumPy íƒ€ì… ì§ë ¬í™” ì—ëŸ¬
   - í•´ê²°: `_convert_to_serializable()` ë©”ì„œë“œ ìˆ˜ì •

### í˜„ì¬ ìˆ˜ì • ì‚¬í•­
2. **KeyError 'roc_auc' ë²„ê·¸** (2024-12-30)
   - íŒŒì¼: [KEYERROR_FIX_SUMMARY.md](KEYERROR_FIX_SUMMARY.md) (ì´ ë¬¸ì„œ)
   - ë¬¸ì œ: Nested metrics dictionary ì ‘ê·¼ ì—ëŸ¬
   - í•´ê²°: `run_single_experiment()` ë©”ì„œë“œ ìˆ˜ì •

---

## ì‚¬ìš© ë°©ë²•

ì´ì œ ì „ì²´ ì‹¤í—˜ì„ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ (100 epochs, ëª¨ë“  ablation studies)
python multivariate_mae_experiments.py
```

ë˜ëŠ” ë¹ ë¥¸ í…ŒìŠ¤íŠ¸:

```bash
# ë‹¨ë… í…ŒìŠ¤íŠ¸ (3 epochs)
python test_experiment_fix.py
```

ê²°ê³¼ëŠ” ë‹¤ìŒ ìœ„ì¹˜ì— ì €ì¥ë©ë‹ˆë‹¤:
```
experiment_results/YYYYMMDD_HHMMSS/
â”œâ”€â”€ experiment_results.json  # âœ… ì •ìƒ ì €ì¥ë¨
â”œâ”€â”€ hyperparameter_comparison.png
â”œâ”€â”€ ablation_comparison.png
â”œâ”€â”€ masking_strategy_comparison.png
â””â”€â”€ performance_heatmap.png
```

---

## íƒ€ì„ë¼ì¸

- **ì—ëŸ¬ ë°œê²¬**: 2024-12-30
- **ì—ëŸ¬ ë³´ê³ **: 2024-12-30 (ì‚¬ìš©ì ì œë³´)
- **ì›ì¸ ë¶„ì„**: 2024-12-30
- **ìˆ˜ì • ì™„ë£Œ**: 2024-12-30
- **í…ŒìŠ¤íŠ¸ ì™„ë£Œ**: 2024-12-30
- **ë¬¸ì„œí™” ì™„ë£Œ**: 2024-12-30

---

## ìš”ì•½

| í•­ëª© | ìƒíƒœ |
|------|------|
| ì—ëŸ¬ íƒ€ì… | `KeyError: 'roc_auc'` |
| ìˆ˜ì • ë°©ë²• | Nested dictionary êµ¬ì¡°ì— ë§ê²Œ ì ‘ê·¼ ê²½ë¡œ ìˆ˜ì • |
| ìˆ˜ì • íŒŒì¼ | `multivariate_mae_experiments.py` (lines 1071-1075) |
| í…ŒìŠ¤íŠ¸ ìƒíƒœ | âœ… í†µê³¼ |
| ë¬¸ì„œí™” | âœ… ì™„ë£Œ |
| ì‚¬ìš© ê°€ëŠ¥ | âœ… ê°€ëŠ¥ |

**ëª¨ë“  ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!** ğŸ‰
